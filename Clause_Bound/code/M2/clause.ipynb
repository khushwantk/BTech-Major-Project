{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import fileinput\n",
    "import functools\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def join_tiny_clauses_with_next(clauses):\n",
    "    # [[\"This\", \"is\", \"just\"], [\"because\"], [\"it\", \"is\", \"so\"]]\n",
    "    # [[\"This\", \"is\", \"just\"], [\"because\", \"it\", \"is\", \"so\"]]\n",
    "    clauses_rev = iter(reversed(clauses))\n",
    "\n",
    "    fixed = []\n",
    "    for clause in clauses_rev:\n",
    "        if len(clause) < 2 and fixed:\n",
    "            fixed[-1] = clause + fixed[-1]\n",
    "            continue\n",
    "        fixed.append(clause)\n",
    "    return list(reversed(fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_shuffler(clauses):\n",
    "    def all_punctuation(clause):\n",
    "        return not any(\n",
    "            set(word)\n",
    "            & set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\")\n",
    "            for word in clause\n",
    "        )\n",
    "\n",
    "    queue = []\n",
    "    to_prepend = None\n",
    "    for clause in clauses:\n",
    "        if not queue:\n",
    "            queue.append(clause)\n",
    "            continue\n",
    "        if clause[0] == '\"' and all_punctuation(clause):\n",
    "            if to_prepend:\n",
    "                queue.append(to_prepend + clause)\n",
    "                to_prepend = None\n",
    "            else:\n",
    "                queue[-1].append('\"')\n",
    "                to_prepend = clause[1:]\n",
    "        elif all_punctuation(clause):\n",
    "            if to_prepend:\n",
    "                queue[-1].extend(to_prepend + clause)\n",
    "                to_prepend = None\n",
    "            else:\n",
    "                queue[-1].extend(clause)\n",
    "        elif all_punctuation([clause[0]]) and clause[0] != '\"':\n",
    "            if to_prepend:\n",
    "                queue.append(to_prepend + clause)\n",
    "                to_prepend = None\n",
    "            elif len(clause) <= 3:\n",
    "                queue[-1].append(clause[0])\n",
    "                to_prepend = clause[1:]\n",
    "            else:\n",
    "                queue[-1].append(clause[0])\n",
    "                queue.append(clause[1:])\n",
    "        elif len(clause) <= 3:  # Laura forced me, could be changed two 2\n",
    "            if to_prepend:\n",
    "                queue.append(to_prepend + clause)\n",
    "                to_prepend = None\n",
    "            else:\n",
    "                to_prepend = clause\n",
    "        else:\n",
    "            if to_prepend:\n",
    "                clause = to_prepend + clause\n",
    "                to_prepend = None\n",
    "            queue.append(clause)\n",
    "    if to_prepend:\n",
    "        queue[-1].extend(to_prepend)\n",
    "\n",
    "    if queue[0] == ['\"'] and len(queue) > 1:\n",
    "        queue.pop(0)\n",
    "        queue[0].insert(0, '\"')\n",
    "\n",
    "    return queue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in fileinput.input():\n",
    "    data = json.loads(line)\n",
    "    tokens, tags = list(zip(*data[\"tokens\"]))\n",
    "    doc = nlp.tokenizer.tokens_from_list(\n",
    "        [token.replace(\"(\", \"-LBR-\").replace(\")\", \"-RBR-\") for token in tokens]\n",
    "    )\n",
    "    nlp.tagger(doc)  # benepar assumes a tagged doc\n",
    "    nlp.parser(doc)  # needed this to split into sentences\n",
    "    benepar(doc)\n",
    "\n",
    "    assert len(doc) == len(\n",
    "        tags\n",
    "    ), f\"size mismatch: tags:{tags} {len(tags)}, text:{doc} {len(doc)}\"\n",
    "\n",
    "    all_clauses = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        t = Tree.fromstring(sent._.parse_string)\n",
    "        all_leaves = t.leaves()\n",
    "        wordidx2treeidx = [None] * len(all_leaves)\n",
    "        for i, subtree in enumerate(t.subtrees()):\n",
    "            if subtree.label() not in (\"S\", \"SBAR\", \"SBARQ\", \"SINV\", \"SQ\"):\n",
    "                continue\n",
    "            leaves = subtree.leaves()\n",
    "            whole_tree = \" \".join(all_leaves)\n",
    "            index = whole_tree.index(\" \".join(leaves))\n",
    "            num_prewords = whole_tree[:index].count(\n",
    "                \" \"\n",
    "            )  # how many words before this subtree\n",
    "            for j in range(num_prewords, num_prewords + len(leaves)):\n",
    "                wordidx2treeidx[j] = i\n",
    "        # Now we have a list that corresponds to the words and tells us which\n",
    "        # word belongs to which subtree\n",
    "\n",
    "        leaf_agenda = all_leaves[::-1]\n",
    "\n",
    "        clauses = []\n",
    "        for stidx, grouper in groupby(wordidx2treeidx):\n",
    "            grouper = list(grouper)\n",
    "            clauses.append([leaf_agenda.pop() for _ in range(len(list(grouper)))])\n",
    "\n",
    "        # Done with naive clause segmentation in sentence based on constituency trees\n",
    "\n",
    "        clauses = punctuation_shuffler(clauses)\n",
    "        clauses = join_tiny_clauses_with_next(clauses)\n",
    "\n",
    "        all_clauses.extend(clauses)\n",
    "\n",
    "    # align tags with the tokens in the clauses:\n",
    "    assert sum(len(clause) for clause in all_clauses) == len(tags)\n",
    "    rtags = list(reversed(tags))\n",
    "    if \"clauses\" in data:\n",
    "        key = \"clauses-predicted\"\n",
    "    else:\n",
    "        key = \"clauses\"\n",
    "\n",
    "    data[key] = [\n",
    "        [\n",
    "            (token.replace(\"-LBR-\", \"(\").replace(\"-RBR-\", \")\"), rtags.pop())\n",
    "            for token in clause\n",
    "        ]\n",
    "        for clause in all_clauses\n",
    "    ]\n",
    "    assert not rtags\n",
    "\n",
    "    data[\"steps\"].append(\"clausify\")\n",
    "    print(json.dumps(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
